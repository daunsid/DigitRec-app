{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d183a75",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcfbae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35932177",
   "metadata": {},
   "outputs": [],
   "source": [
    "sld_path = path.Path(\"../data\")\n",
    "#list_dir = os.listdir(sld_path/\"Examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b21d6a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(loader):\n",
    "    mean=0\n",
    "    std=0\n",
    "    nb_samples=0\n",
    "    for data in loader:\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "    \n",
    "    return transforms.Normalize(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4a3e2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(128,128)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "851241ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sign_lang(Dataset):\n",
    "    def __init__(self, path, transforms= None):\n",
    "        self.transforms = transforms\n",
    "        self.path = path\n",
    "        \n",
    "        self.data={cl:([(PIL.Image.open(path/cl/im),\n",
    "                         int(cl)) for im in os.listdir(path/cl)]) \n",
    "                   for cl in os.listdir(path)}\n",
    "        self.classes = self.data.keys()\n",
    "        self.samp=[]\n",
    "        for i in self.data:\n",
    "            for e in self.data[i]:\n",
    "                self.samp.append(e)\n",
    "        self.targ = [self.samp[i][1] for i in range(len(self.samp))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samp)\n",
    "    \n",
    "    def targets(self):\n",
    "        return self.targ\n",
    "    \n",
    "    def samples(self):\n",
    "        return self.samp\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samp[index][0], self.samp[index][1]\n",
    "        \n",
    "        if self.transforms:    \n",
    "            sample = self.transforms(sample[0]), sample[1]\n",
    "        return sample\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{__class__.__name__}:\\nNum of datapoints: {len(self.samp)}\\nRoot location: {self.path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6156bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkDataset():\n",
    "    image_datasets = {x: sign_lang(sld_path/x, transform)\n",
    "                      for x in ['train', 'valid', 'tests']}\n",
    "    return image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "757c10b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    loaders = {x+'_loader': DataLoader(mkDataset()[x], batch_size=10,\n",
    "                                              shuffle=True, num_workers=0)\n",
    "                      for x in ['train', 'valid', 'tests']}\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ab53ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl=loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e64a7e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 128, 128])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dl[\"tests_loader\"]))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e512d005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 1712, 'valid': 300, 'tests': 50}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes = {x: len(mkDataset()[x]) for x in ['train', 'valid', 'tests']}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "356a9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms.Normalize(mean, std)transforms.ToTensor()(PIL.Image.open(sld_path/\"Examples/example_2.JPG\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f1d0a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bf00ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import copy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "662094b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_layer():\n",
    "    \n",
    "    input_shape = next(iter(loadData()[\"train_loader\"]))[0].shape\n",
    "    return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "28c95e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignNet(nn.Module):\n",
    "    def __init__(self, in_channels=input_layer()[1], out_channels=6):\n",
    "        super(SignNet, self).__init__()\n",
    "        \n",
    "        self.Conv1 = nn.Conv2d(in_channels, out_channels, 5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.Conv2 = nn.Conv2d(out_channels, 16, 5)\n",
    "        self.Conv3 = nn.Conv2d(16, 46, 5)\n",
    "        self.fc1 = nn.Linear(46*12*12, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.Conv1(x)))\n",
    "        x = self.pool(F.relu(self.Conv2(x)))\n",
    "        x = self.pool(F.relu(self.Conv3(x)))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "def softmax(x): return torch.exp(x)/torch.exp(x).sum(dim=1,keepdim=True)\n",
    "\n",
    "def criterion(prediction, true_value):\n",
    "    sm_pred = softmax(prediction)\n",
    "    idx = range(len(prediction))\n",
    "    return -torch.log(sm_pred[idx,true_value]).mean()\n",
    "\n",
    "def get_lr(): return 0.01\n",
    "def get_epoch(): return 20\n",
    "\n",
    "model = SignNet()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=get_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "395c7d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer):\n",
    "    \n",
    "    since = time.time()\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(get_epoch()):\n",
    "        \n",
    "        print(f'epoch {epoch}/{get_epoch()-1}')\n",
    "        print('=' * 15)\n",
    "            \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train_loader', 'valid_loader']:\n",
    "            if phase == 'train_loader':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in loadData()[phase]:\n",
    "                #inputs = inputs.to(device)\n",
    "                #labels = labels.to(device)\n",
    "                \n",
    "            \n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train_loader'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "            \n",
    "                        # backward\n",
    "                    if phase == 'train_loader':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            #if phase == 'train_loader':\n",
    "             #   scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase[:5]]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase[:5]]\n",
    "            \n",
    "            print(f'{phase[:5]} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'valid_loader' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "                \n",
    "\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "50e773e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/19\n",
      "===============\n",
      "train Loss: 2.3032 Acc: 0.1011\n",
      "valid Loss: 2.3011 Acc: 0.1000\n",
      "\n",
      "epoch 1/19\n",
      "===============\n",
      "train Loss: 2.2998 Acc: 0.1232\n",
      "valid Loss: 2.2961 Acc: 0.1567\n",
      "\n",
      "epoch 2/19\n",
      "===============\n",
      "train Loss: 2.2800 Acc: 0.1787\n",
      "valid Loss: 2.2462 Acc: 0.2467\n",
      "\n",
      "epoch 3/19\n",
      "===============\n",
      "train Loss: 1.8959 Acc: 0.3458\n",
      "valid Loss: 2.7986 Acc: 0.3200\n",
      "\n",
      "epoch 4/19\n",
      "===============\n",
      "train Loss: 1.1517 Acc: 0.6121\n",
      "valid Loss: 1.7746 Acc: 0.4167\n",
      "\n",
      "epoch 5/19\n",
      "===============\n",
      "train Loss: 0.6997 Acc: 0.7763\n",
      "valid Loss: 0.9173 Acc: 0.7067\n",
      "\n",
      "epoch 6/19\n",
      "===============\n",
      "train Loss: 0.4460 Acc: 0.8540\n",
      "valid Loss: 0.4698 Acc: 0.8667\n",
      "\n",
      "epoch 7/19\n",
      "===============\n",
      "train Loss: 0.3176 Acc: 0.9042\n",
      "valid Loss: 0.4734 Acc: 0.8367\n",
      "\n",
      "epoch 8/19\n",
      "===============\n",
      "train Loss: 0.2155 Acc: 0.9346\n",
      "valid Loss: 0.5422 Acc: 0.8267\n",
      "\n",
      "epoch 9/19\n",
      "===============\n",
      "train Loss: 0.1702 Acc: 0.9527\n",
      "valid Loss: 0.3075 Acc: 0.8933\n",
      "\n",
      "epoch 10/19\n",
      "===============\n",
      "train Loss: 0.1024 Acc: 0.9696\n",
      "valid Loss: 0.2601 Acc: 0.9133\n",
      "\n",
      "epoch 11/19\n",
      "===============\n",
      "train Loss: 0.0969 Acc: 0.9708\n",
      "valid Loss: 0.3528 Acc: 0.8900\n",
      "\n",
      "epoch 12/19\n",
      "===============\n",
      "train Loss: 0.0598 Acc: 0.9819\n",
      "valid Loss: 0.3307 Acc: 0.9133\n",
      "\n",
      "epoch 13/19\n",
      "===============\n",
      "train Loss: 0.0365 Acc: 0.9895\n",
      "valid Loss: 0.2940 Acc: 0.9333\n",
      "\n",
      "epoch 14/19\n",
      "===============\n",
      "train Loss: 0.0182 Acc: 0.9965\n",
      "valid Loss: 0.3068 Acc: 0.9233\n",
      "\n",
      "epoch 15/19\n",
      "===============\n",
      "train Loss: 0.0125 Acc: 0.9971\n",
      "valid Loss: 0.2820 Acc: 0.9300\n",
      "\n",
      "epoch 16/19\n",
      "===============\n",
      "train Loss: 0.0052 Acc: 0.9994\n",
      "valid Loss: 0.2888 Acc: 0.9333\n",
      "\n",
      "epoch 17/19\n",
      "===============\n",
      "train Loss: 0.0021 Acc: 1.0000\n",
      "valid Loss: 0.3130 Acc: 0.9367\n",
      "\n",
      "epoch 18/19\n",
      "===============\n",
      "train Loss: 0.0017 Acc: 1.0000\n",
      "valid Loss: 0.3015 Acc: 0.9333\n",
      "\n",
      "epoch 19/19\n",
      "===============\n",
      "train Loss: 0.0015 Acc: 1.0000\n",
      "valid Loss: 0.3162 Acc: 0.9333\n",
      "\n",
      "Training complete in 13m 55s\n",
      "Best val Acc: 0.9367\n"
     ]
    }
   ],
   "source": [
    "mod=train_model(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ef240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction,label): \n",
    "    pred_sm = softmax(prediction).argmax(axis=1)\n",
    "    count = label == pred_sm\n",
    "    acc = count.float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bf6e871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(dl_model, export_dir, exported_model=\"model.pth\"):\n",
    "    if not os.path.isdir(export_dir):\n",
    "        export_path = os.mkdir(export_dir)\n",
    "    model_path = path.Path(export_dir)/exported_model\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"-%Y-%m-%d-%H-%M-%S\")\n",
    "    model_path = path.Path(str(model_path)+timestamp)\n",
    "    \n",
    "    torch.save(dl_model.state_dict(), model_path)\n",
    "    \n",
    "def inference(img, model_path):\n",
    "    model = SignNet()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    #with model.eval():\n",
    "    infer_img = transform(PIL.Image.open(img))\n",
    "    infer_img=infer_img.view(1, infer_img.shape[0], infer_img.shape[1], infer_img.shape[2])\n",
    "    outputs = softmax(model(infer_img))\n",
    "    probability, classes = torch.max(outputs, 1)\n",
    "    \n",
    "    return f'Prediction: {classes.item()}; Probability: {probability.item()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2579bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model(mod, \"../outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2ab0ec7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_path = (path.Path(\"../data/Examples/example_4.JPG\"))\n",
    "os.path.isfile(im_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f76cf2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.pth-2022-05-02-00-11-03'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../outputs\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c22f49a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prediction: 4; Probability: 0.9994024634361267'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(im_path, \"../outputs/\"+os.listdir(\"../outputs\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "176aa448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../data/Examples/example_4.JPG')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be617588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
